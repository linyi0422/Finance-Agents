{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Qwen Sentiment 模型演示\n",
        "\n",
        "本笔记演示如何在本地环境中加载微调后的 Qwen 情感模型，并对新闻文本进行情感打分。运行前请先完成以下准备：\n",
        "\n",
        "- 在 Conda/venv 中安装 `torch`, `transformers`, `peft`, `pandas` 等依赖。\n",
        "- 将基础模型目录和微调后的 LoRA 权重目录放在本地磁盘，并记录实际路径。\n",
        "- 如果使用 CPU 推理，请确认显存/内存允许（可将下方的 `torch_dtype` 调整为 `torch.float32` 并强制 `device_map={\"\": \"cpu\"}`）。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# TODO: 将下面的路径替换为你本地的实际目录\n",
        "BASE_MODEL_DIR = Path(r\"C:\\\\Users\\\\nings\\\\Downloads\\\\居丽叶简历项目3：股票投资顾问Agent\\\\Finance\\\\Qwen\")\n",
        "FINETUNED_MODEL_DIR = Path(r\"C:\\\\Users\\\\nings\\\\Downloads\\\\居丽叶简历项目3：股票投资顾问Agent\\\\Finance\\\\qwen_sentiment_model\")\n",
        "\n",
        "if not BASE_MODEL_DIR.exists():\n",
        "    raise FileNotFoundError(f\"基础模型目录不存在: {BASE_MODEL_DIR}\")\n",
        "\n",
        "if not FINETUNED_MODEL_DIR.exists():\n",
        "    raise FileNotFoundError(f\"LoRA 模型目录不存在: {FINETUNED_MODEL_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "TORCH_DTYPE = torch.float16  # 如需 CPU 推理，可改为 torch.float32\n",
        "DEVICE_MAP = \"auto\"          # CPU 推理可改为 {\"\": \"cpu\"}\n",
        "\n",
        "\n",
        "def load_sentiment_model(base_model_dir: Path, finetuned_model_dir: Path):\n",
        "    \"\"\"从本地路径加载基础模型与 LoRA 权重。\"\"\"\n",
        "    print(\"正在加载 tokenizer …\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(finetuned_model_dir)\n",
        "\n",
        "    print(\"正在加载基础模型 …\")\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_dir,\n",
        "        torch_dtype=TORCH_DTYPE,\n",
        "        device_map=DEVICE_MAP,\n",
        "    )\n",
        "\n",
        "    print(\"正在合并 LoRA 权重 …\")\n",
        "    model = PeftModel.from_pretrained(base_model, finetuned_model_dir)\n",
        "    model.eval()\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def create_prompt(text: str, stock_symbol: str = \"STOCK\") -> str:\n",
        "    system_prompt = (\n",
        "        \"Forget all your previous instructions. You are a financial expert with stock \"\n",
        "        \"recommendation experience. Based on a specific stock, score for range from 1 to 5, \"\n",
        "        \"where 1 is negative, 2 is somewhat negative, 3 is neutral, 4 is somewhat positive, \"\n",
        "        \"5 is positive. 1 summarized news will be passed in each time, you will give score in \"\n",
        "        \"format as shown below in the response from assistant.\"\n",
        "    )\n",
        "\n",
        "    few_shot_examples = (\n",
        "        \"User: News to Stock Symbol -- AAPL: Apple (AAPL) increase 22%\\n\"\n",
        "        \"Assistant: 5\\n\\n\"\n",
        "        \"User: News to Stock Symbol -- AAPL: Apple (AAPL) price decreased 30%\\n\"\n",
        "        \"Assistant: 1\\n\\n\"\n",
        "        \"User: News to Stock Symbol -- AAPL: Apple (AAPL) announced iPhone 15\\n\"\n",
        "        \"Assistant: 4\\n\\n\"\n",
        "    )\n",
        "\n",
        "    current_query = f\"User: News to Stock Symbol -- {stock_symbol}: {text}\\nAssistant:\"\n",
        "\n",
        "    return f\"System: {system_prompt}\\n\\n{few_shot_examples}{current_query}\"\n",
        "\n",
        "\n",
        "def predict_sentiment(model, tokenizer, text: str, stock_symbol: str = \"STOCK\") -> int | None:\n",
        "    prompt = create_prompt(text, stock_symbol)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=5,\n",
        "            do_sample=False,\n",
        "            temperature=0.1,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    assistant_response = generated_text.split(\"Assistant:\")[-1].strip()\n",
        "\n",
        "    try:\n",
        "        sentiment_score = int(assistant_response.split()[0])\n",
        "        if 1 <= sentiment_score <= 5:\n",
        "            return sentiment_score\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model, tokenizer = load_sentiment_model(BASE_MODEL_DIR, FINETUNED_MODEL_DIR)\n",
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_news = [\n",
        "    (\"Apple reported strong quarterly earnings with revenue growth of 15%\", \"AAPL\"),\n",
        "    (\"Apple faces supply chain disruptions and production delays\", \"AAPL\"),\n",
        "    (\"Tesla delivers record number of vehicles in Q4\", \"TSLA\"),\n",
        "]\n",
        "\n",
        "results = []\n",
        "for text, symbol in sample_news:\n",
        "    score = predict_sentiment(model, tokenizer, text, symbol)\n",
        "    results.append({\n",
        "        \"stock_symbol\": symbol,\n",
        "        \"news\": text,\n",
        "        \"predicted_sentiment\": score,\n",
        "    })\n",
        "\n",
        "results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 下一步\n",
        "\n",
        "- 如需批量预测，可将上面的 `sample_news` 替换为来自 CSV/DataFrame 的真实新闻数据。\n",
        "- 风险模型的使用方式类似，只需将提示函数字段调整为风险评分说明，并加载 `qwen_risk_model` 权重。\n",
        "- 推理结束后记得释放 GPU 显存，可执行 `del model; torch.cuda.empty_cache()`。\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
